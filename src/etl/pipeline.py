"""
üîÑ ETL Pipeline Module
======================

Pipeline completo de ETL que integra:
- Extracci√≥n (DataExtractor)
- Transformaci√≥n (DataTransformer)
- Carga (DataLoader)
- Validaci√≥n (DataValidator)

Autor: Elizabeth D√≠az Familia
"""

import pandas as pd
from typing import Dict, Any, Optional
from datetime import datetime
import json

from .extractor import DataExtractor
from .transformer import DataTransformer
from .loader import DataLoader
from .validator import DataValidator


class ETLPipeline:
    """
    Pipeline completo de ETL
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Inicializar el pipeline ETL
        
        Args:
            config: Configuraci√≥n del pipeline
        """
        self.config = config or {}
        self.extractor = DataExtractor(config.get('extractor', {}))
        self.transformer = DataTransformer(config.get('transformer', {}))
        self.loader = DataLoader(config.get('output_dir', 'data/processed'))
        self.validator = DataValidator()
        
        self.execution_log = []
        self.start_time = None
        self.end_time = None
        
    def log_step(self, step: str, status: str, details: str = ""):
        """
        Registrar paso del pipeline
        
        Args:
            step: Nombre del paso
            status: Estado (success/error/warning)
            details: Detalles adicionales
        """
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'step': step,
            'status': status,
            'details': details
        }
        self.execution_log.append(log_entry)
        
        icon = "‚úÖ" if status == "success" else "‚ùå" if status == "error" else "‚ö†Ô∏è"
        print(f"{icon} {step}: {details}")
    
    def extract_data(self, source_type: str = 'mock', **kwargs) -> pd.DataFrame:
        """
        Paso 1: Extraer datos
        
        Args:
            source_type: Tipo de fuente ('mock', 'csv', 'excel', 'api')
            **kwargs: Argumentos espec√≠ficos del tipo de fuente
            
        Returns:
            DataFrame con datos extra√≠dos
        """
        print("\n" + "=" * 70)
        print("üì• PASO 1: EXTRACCI√ìN DE DATOS")
        print("=" * 70)
        
        try:
            if source_type == 'mock':
                df = self.extractor.generate_mock_data(kwargs.get('n_records', 1000))
            elif source_type == 'csv':
                df = self.extractor.extract_from_csv(kwargs.get('filepath'))
            elif source_type == 'excel':
                df = self.extractor.extract_from_excel(
                    kwargs.get('filepath'),
                    kwargs.get('sheet_name', 0)
                )
            elif source_type == 'api':
                api_data = self.extractor.extract_from_api(
                    kwargs.get('url'),
                    kwargs.get('params'),
                    kwargs.get('headers')
                )
                df = pd.DataFrame(api_data)
            else:
                raise ValueError(f"Tipo de fuente no soportado: {source_type}")
            
            self.log_step("Extracci√≥n", "success", f"{len(df):,} registros extra√≠dos")
            return df
            
        except Exception as e:
            self.log_step("Extracci√≥n", "error", str(e))
            raise
    
    def transform_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Paso 2: Transformar datos
        
        Args:
            df: DataFrame a transformar
            
        Returns:
            DataFrame transformado
        """
        print("\n" + "=" * 70)
        print("üîß PASO 2: TRANSFORMACI√ìN DE DATOS")
        print("=" * 70)
        
        try:
            df_transformed = self.transformer.apply_all_transformations(df)
            
            self.log_step(
                "Transformaci√≥n",
                "success",
                f"{len(df_transformed):,} registros, {len(df_transformed.columns)} columnas"
            )
            return df_transformed
            
        except Exception as e:
            self.log_step("Transformaci√≥n", "error", str(e))
            raise
    
    def validate_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Paso 3: Validar datos
        
        Args:
            df: DataFrame a validar
            
        Returns:
            Resultados de validaci√≥n
        """
        print("\n" + "=" * 70)
        print("‚úÖ PASO 3: VALIDACI√ìN DE DATOS")
        print("=" * 70)
        
        try:
            validation_results = self.validator.run_full_validation(df)
            
            if len(validation_results['errors']) == 0:
                self.log_step("Validaci√≥n", "success", "Todos los checks pasaron")
            else:
                self.log_step(
                    "Validaci√≥n",
                    "warning",
                    f"{len(validation_results['errors'])} errores encontrados"
                )
            
            return validation_results
            
        except Exception as e:
            self.log_step("Validaci√≥n", "error", str(e))
            raise
    
    def load_data(self, df: pd.DataFrame, base_filename: str = 'telecom_churn') -> Dict[str, str]:
        """
        Paso 4: Cargar datos
        
        Args:
            df: DataFrame a cargar
            base_filename: Nombre base de archivos
            
        Returns:
            Diccionario con rutas de archivos guardados
        """
        print("\n" + "=" * 70)
        print("üíæ PASO 4: CARGA DE DATOS")
        print("=" * 70)
        
        try:
            paths = self.loader.save_all_formats(df, base_filename)
            
            self.log_step(
                "Carga",
                "success",
                f"{len(paths)} archivos guardados"
            )
            return paths
            
        except Exception as e:
            self.log_step("Carga", "error", str(e))
            raise
    
    def run_full_pipeline(self, source_type: str = 'mock', 
                         base_filename: str = 'telecom_churn_processed',
                         **extract_kwargs) -> Dict[str, Any]:
        """
        Ejecutar pipeline completo
        
        Args:
            source_type: Tipo de fuente de datos
            base_filename: Nombre base para archivos de salida
            **extract_kwargs: Argumentos para la extracci√≥n
            
        Returns:
            Diccionario con resultados del pipeline
        """
        self.start_time = datetime.now()
        
        print("\n" + "‚ïê" * 70)
        print("üöÄ INICIANDO PIPELINE ETL COMPLETO")
        print("‚ïê" * 70)
        print(f"‚è∞ Inicio: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("‚ïê" * 70)
        
        try:
            # Paso 1: Extracci√≥n
            df_raw = self.extract_data(source_type, **extract_kwargs)
            
            # Paso 2: Transformaci√≥n
            df_transformed = self.transform_data(df_raw)
            
            # Paso 3: Validaci√≥n
            validation_results = self.validate_data(df_transformed)
            
            # Paso 4: Carga
            output_paths = self.load_data(df_transformed, base_filename)
            
            self.end_time = datetime.now()
            duration = (self.end_time - self.start_time).total_seconds()
            
            # Resumen final
            results = {
                'success': True,
                'start_time': self.start_time.isoformat(),
                'end_time': self.end_time.isoformat(),
                'duration_seconds': duration,
                'records_extracted': len(df_raw),
                'records_loaded': len(df_transformed),
                'columns_original': len(df_raw.columns),
                'columns_final': len(df_transformed.columns),
                'validation_results': validation_results,
                'output_files': output_paths,
                'execution_log': self.execution_log
            }
            
            # Guardar log de ejecuci√≥n
            log_path = self.loader.output_dir / f'{base_filename}_pipeline_log.json'
            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=4, ensure_ascii=False)
            
            print("\n" + "‚ïê" * 70)
            print("üéâ PIPELINE COMPLETADO EXITOSAMENTE")
            print("‚ïê" * 70)
            print(f"‚è±Ô∏è Duraci√≥n: {duration:.2f} segundos")
            print(f"üìä Registros procesados: {len(df_raw):,} ‚Üí {len(df_transformed):,}")
            print(f"üìã Columnas: {len(df_raw.columns)} ‚Üí {len(df_transformed.columns)}")
            print(f"üíæ Archivos generados: {len(output_paths)}")
            print("‚ïê" * 70)
            
            return results
            
        except Exception as e:
            self.end_time = datetime.now()
            duration = (self.end_time - self.start_time).total_seconds()
            
            error_results = {
                'success': False,
                'error': str(e),
                'start_time': self.start_time.isoformat(),
                'end_time': self.end_time.isoformat(),
                'duration_seconds': duration,
                'execution_log': self.execution_log
            }
            
            print("\n" + "‚ïê" * 70)
            print("‚ùå PIPELINE FALLIDO")
            print("‚ïê" * 70)
            print(f"üö® Error: {str(e)}")
            print(f"‚è±Ô∏è Duraci√≥n: {duration:.2f} segundos")
            print("‚ïê" * 70)
            
            return error_results
    
    def get_execution_summary(self) -> Dict[str, Any]:
        """
        Obtener resumen de la ejecuci√≥n
        
        Returns:
            Diccionario con resumen
        """
        if not self.execution_log:
            return {"message": "Pipeline no ejecutado"}
        
        summary = {
            'total_steps': len(self.execution_log),
            'successful_steps': sum(1 for log in self.execution_log if log['status'] == 'success'),
            'failed_steps': sum(1 for log in self.execution_log if log['status'] == 'error'),
            'warnings': sum(1 for log in self.execution_log if log['status'] == 'warning'),
            'execution_log': self.execution_log
        }
        
        if self.start_time and self.end_time:
            summary['duration_seconds'] = (self.end_time - self.start_time).total_seconds()
        
        return summary


if __name__ == "__main__":
    # Ejemplo de uso
    print("üîÑ ETL Pipeline - Ejemplo de Uso")
    print("=" * 70)
    
    # Configuraci√≥n del pipeline
    config = {
        'output_dir': 'data/processed',
        'extractor': {},
        'transformer': {}
    }
    
    # Crear pipeline
    pipeline = ETLPipeline(config)
    
    # Ejecutar pipeline completo con datos mock
    results = pipeline.run_full_pipeline(
        source_type='mock',
        n_records=500,
        base_filename='telecom_churn_demo'
    )
    
    # Mostrar resumen
    if results['success']:
        print("\n‚úÖ Pipeline ejecutado exitosamente")
        print(f"üìÅ Archivos generados:")
        for file_type, path in results['output_files'].items():
            print(f"   - {file_type}: {path}")
    else:
        print(f"\n‚ùå Pipeline fall√≥: {results.get('error')}")
